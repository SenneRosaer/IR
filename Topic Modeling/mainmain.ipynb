{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tmtoon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import time\n",
    "import random\n",
    "import unidecode\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "%load_ext line_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"news_dataset.csv\")\n",
    "df = df[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    washington eeaaaaeeeace    congressional repub...\n",
      "1    after the bullet shells get counted the blood ...\n",
      "2    when walt disneys bambi opened in  critics pra...\n",
      "3    death may be the great equalizer but it isnt n...\n",
      "4    seoul south korea     north koreas leader kim ...\n",
      "Name: content, dtype: object\n",
      "0    washington eeaaaaeeeace congressional republic...\n",
      "1     bullet shells get counted blood dries votive ...\n",
      "2     walt disneys bambi opened critics praised spa...\n",
      "3    death may great equalizer isnt necessarily eve...\n",
      "4    seoul south korea north koreas leader kim said...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# remove accents, special characters and lowercase\n",
    "# https://stackoverflow.com/questions/37926248/how-to-remove-accents-from-values-in-columns\n",
    "df.content = df.content.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8').str.lower().str.replace('[^a-z ]', '')\n",
    "print(df.content.head())\n",
    "\n",
    "# remove stop words\n",
    "# https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "stopwords = stopwords.words('english')\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(stopwords))\n",
    "df['content'] = df['content'].str.replace(pat, '')\n",
    "\n",
    "\n",
    "# remove multiple spaces in a\n",
    "df['content'] = df['content'].str.replace(r'\\s+', ' ')\n",
    "df = df[df['content'].notna()]\n",
    "print(df.content.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab_set = set()\n",
    "\n",
    "def compute_dict(input):\n",
    "    result = dict()\n",
    "\n",
    "    for word in input.split(' '):\n",
    "        if word != '':\n",
    "            vocab_set.add(word)\n",
    "            if word in result:\n",
    "                result[word] += 1\n",
    "            else:\n",
    "                result[word] = 1\n",
    "    return result\n",
    "\n",
    "df['content_dict'] = df['content'].apply(lambda content: compute_dict(content))\n",
    "vocab = list(vocab_set)\n",
    "\n",
    "tmp ={}\n",
    "count = 0\n",
    "for word in vocab:\n",
    "    tmp[word] = count\n",
    "    count += 1\n",
    "vocab = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Toevoegen van extra column met index ineens\n",
    "def remove_dupes(input):\n",
    "    output = list(set(input.split(' ')))\n",
    "    if '' in output:\n",
    "        output.remove('')\n",
    "    for i, value in enumerate(output):\n",
    "        output[i] = vocab[value]\n",
    "    return output\n",
    "df['content_list'] = df['content'].apply(lambda content: remove_dupes(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "numberOfTopics = 20\n",
    "topics = [i for i in range(0, numberOfTopics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "wordToTopic = numpy.zeros((len(vocab), numberOfTopics))\n",
    "documentToTopic = numpy.zeros((len(df.content), numberOfTopics))\n",
    "topicAssignment = []\n",
    "words_in_topic = [0 for i in range(numberOfTopics)]\n",
    "words_in_doc = [0 for i in range(len(df.content))]\n",
    "for i, content in enumerate(df.content_list):\n",
    "    tmp = []\n",
    "    for word in content:\n",
    "        topic = numpy.random.choice(topics)\n",
    "        tmp.append(topic)\n",
    "        wordToTopic[word, topic] += 1\n",
    "        documentToTopic[i, topic] += 1\n",
    "        words_in_topic[topic] += 1\n",
    "        words_in_doc[i] += 1\n",
    "    topicAssignment.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "beta = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibs_it(i):\n",
    "    total_unique_word_b = len(vocab) * beta\n",
    "    topic_count_a = numberOfTopics * alpha\n",
    "    for iteration in range(i):\n",
    "        print(\"Iteration:\", iteration + 1)\n",
    "        \n",
    "        for doc_index, word_list in enumerate(df.content_list):\n",
    "            document_words = documentToTopic[doc_index]\n",
    "            TA_d = topicAssignment[doc_index]\n",
    "            WID = words_in_doc[doc_index] - 1 + topic_count_a\n",
    "            \n",
    "            \n",
    "            for word_index, word in enumerate(word_list):\n",
    "                TA = TA_d[word_index]\n",
    "                documentToTopic[doc_index, TA] -= 1\n",
    "                wordToTopic[word, TA] -= 1\n",
    "                words_in_topic[TA] -= 1\n",
    "                \n",
    "                tmp = 0\n",
    "                new_topic = 0\n",
    "                \n",
    "                count_word = wordToTopic[word]\n",
    "                for topic in topics:\n",
    "                    p = ((count_word[topic] + beta)/(words_in_topic[topic] + total_unique_word_b) * ((document_words[topic] + alpha)/(WID)))\n",
    "                    \n",
    "                    if p > tmp:\n",
    "                        tmp = p\n",
    "                        new_topic = topic\n",
    "                    \n",
    "                    \n",
    "                TA_d[word_index] = new_topic\n",
    "                documentToTopic[doc_index, new_topic] += 1\n",
    "                wordToTopic[word, new_topic] += 1\n",
    "                words_in_topic[new_topic] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n"
     ]
    }
   ],
   "source": [
    "%lprun -f gibs_it gibs_it(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "CPU times: user 5.49 s, sys: 3.84 ms, total: 5.49 s\n",
      "Wall time: 5.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gibs_it(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "to_sort = []\n",
    "for i in range(len(documentToTopic)):\n",
    "    to_sort.append((documentToTopic[i][0], i))\n",
    "\n",
    "to_sort.sort(key=lambda t: t[0], reverse=True)\n",
    "result1 = df.content[to_sort[0][1]]\n",
    "result2 = df.content[to_sort[1][1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
