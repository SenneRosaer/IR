{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.86 s, sys: 846 ms, total: 2.71 s\n",
      "Wall time: 1.54 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tmtoon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import nltk\n",
    "import numpy\n",
    "import time\n",
    "import random\n",
    "import unidecode\n",
    "import string\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from numba import njit, jit, cuda\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "%load_ext line_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMOUNT_OF_TOPICS = 10\n",
    "\n",
    "# top % of most common words are removed\n",
    "TOP_K_WORD_REMOVAL = 0\n",
    "\n",
    "# https://stats.stackexchange.com/questions/59684/what-are-typical-values-to-use-for-alpha-and-beta-in-latent-dirichlet-allocation/130876\n",
    "# High alpha: many topics per document [low : 1 : high]\n",
    "ALPHA            = 50 / AMOUNT_OF_TOPICS\n",
    "# High beta: topic has many different words [0 - 1]\n",
    "BETA             = 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.03 s, sys: 666 ms, total: 7.69 s\n",
      "Wall time: 7.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv(\"news_dataset.csv\")\n",
    "#df = df[:1000]\n",
    "df = df[df['content'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    washington eeaaaaeeeace    congressional repub...\n",
      "1    after the bullet shells get counted the blood ...\n",
      "2    when walt disneys bambi opened in  critics pra...\n",
      "3    death may be the great equalizer but it isnt n...\n",
      "4    seoul south korea     north koreas leader kim ...\n",
      "Name: content, dtype: object\n",
      "CPU times: user 18.3 s, sys: 968 ms, total: 19.3 s\n",
      "Wall time: 19.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove accents, special characters and lowercase\n",
    "# https://stackoverflow.com/questions/37926248/how-to-remove-accents-from-values-in-columns\n",
    "df.content = df.content.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8').str.lower().str.replace('[^a-z ]', '')\n",
    "print(df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    washington eeaaaaeeeace    congressional repub...\n",
      "1      bullet shells get counted  blood dries   vot...\n",
      "2     walt disneys bambi opened   critics praised  ...\n",
      "3    death may   great equalizer   isnt necessarily...\n",
      "4    seoul south korea     north koreas leader kim ...\n",
      "Name: content, dtype: object\n",
      "CPU times: user 2min 30s, sys: 19.4 ms, total: 2min 30s\n",
      "Wall time: 2min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove stop words and single letters\n",
    "# https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list.extend(list(string.ascii_lowercase))\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(stopwords_list))\n",
    "df['content'] = df['content'].str.replace(pat, '')\n",
    "print(df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 9.78 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove common words\n",
    "#fdist = FreqDist(sum(df['content'].map(word_tokenize), []))\n",
    "#top_k_words, _ = zip(*fdist.most_common(int(TOP_K_WORD_REMOVAL * len(fdist))))\n",
    "#print(top_k_words)\n",
    "\n",
    "#pat = r'\\b(?:{})\\b'.format('|'.join(top_k_words))\n",
    "\n",
    "#df['content'] = df['content'].str.replace(pat, '')\n",
    "#print(df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 5 µs, total: 5 µs\n",
      "Wall time: 10.5 µs\n",
      "0    washington eeaaaaeeeace congressional republic...\n",
      "1     bullet shells get counted blood dries votive ...\n",
      "2     walt disneys bambi opened critics praised spa...\n",
      "3    death may great equalizer isnt necessarily eve...\n",
      "4    seoul south korea north koreas leader kim said...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "# remove multiple spaces in a\n",
    "df['content'] = df['content'].str.replace(r'\\s+', ' ')\n",
    "print(df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.1 s, sys: 920 ms, total: 27 s\n",
      "Wall time: 27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vocab_set = set()\n",
    "\n",
    "def compute_dict(input):\n",
    "    result = dict()\n",
    "\n",
    "    for word in input.split(' '):\n",
    "        if word != '':\n",
    "            vocab_set.add(word)\n",
    "            if word in result:\n",
    "                result[word] += 1\n",
    "            else:\n",
    "                result[word] = 1\n",
    "    return result\n",
    "\n",
    "df['content_dict'] = df['content'].apply(lambda content: compute_dict(content))\n",
    "vocab = list(vocab_set)\n",
    "\n",
    "tmp ={}\n",
    "count = 0\n",
    "for word in vocab:\n",
    "    tmp[word] = count\n",
    "    count += 1\n",
    "vocab = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.8 s, sys: 164 ms, total: 29 s\n",
      "Wall time: 29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Toevoegen van extra column met index ineens\n",
    "def remove_dupes(input):\n",
    "    output = list(set(input.split(' ')))\n",
    "    if '' in output:\n",
    "        output.remove('')\n",
    "    for i, value in enumerate(output):\n",
    "        output[i] = vocab[value]\n",
    "    return np.array(output)\n",
    "df['content_list'] = df['content'].apply(lambda content: remove_dupes(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 10.7 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "numberOfTopics = AMOUNT_OF_TOPICS\n",
    "topics = [i for i in range(0, numberOfTopics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "wordToTopic = numpy.zeros((len(vocab), numberOfTopics), dtype=numpy.int64)\n",
    "documentToTopic = numpy.zeros((len(df.content), numberOfTopics))\n",
    "topicAssignment = []\n",
    "words_in_topic = numpy.zeros(numberOfTopics, dtype=numpy.int64)\n",
    "words_in_doc = [0 for i in range(len(df.content))]\n",
    "for i, content in enumerate(df.content_list):\n",
    "    tmp = []\n",
    "    for word in content:\n",
    "        topic = numpy.random.choice(topics)\n",
    "        tmp.append(topic)\n",
    "        wordToTopic[word, topic] += 1\n",
    "        documentToTopic[i, topic] += 1\n",
    "        words_in_topic[topic] += 1\n",
    "        words_in_doc[i] += 1\n",
    "    topicAssignment.append(np.array(tmp))\n",
    "    \n",
    "topicAssignment = np.array(topicAssignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "alpha = ALPHA\n",
    "beta = BETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "@njit\n",
    "def calc_fast(TA_d, doc_index, word_index, wordToTopic, word, words_in_topic, total_unique_word_b, document_words, WID):\n",
    "    document_words[TA_d[word_index]] -= 1\n",
    "    wordToTopic[word, TA_d[word_index]] -= 1\n",
    "    words_in_topic[TA_d[word_index]] -= 1\n",
    "                \n",
    "    new_topic = numpy.argmax(np.multiply(np.divide((wordToTopic[word] + beta), \n",
    "                                                   (words_in_topic + total_unique_word_b)), \n",
    "                                         np.divide((document_words + alpha), \n",
    "                                                   (WID))))\n",
    "\n",
    "    TA_d[word_index] = new_topic\n",
    "    document_words[new_topic] += 1\n",
    "    wordToTopic[word, new_topic] += 1\n",
    "    words_in_topic[new_topic] += 1\n",
    "    \n",
    "def gibs_it_sanic(i):\n",
    "    total_unique_word_b = len(vocab) * beta\n",
    "    topic_count_a = numberOfTopics * alpha\n",
    "    for iteration in range(i):\n",
    "        print(\"Iteration:\", iteration + 1)\n",
    "        \n",
    "        for doc_index, word_list in enumerate(df.content_list):\n",
    "            document_words = documentToTopic[doc_index]\n",
    "            WID = words_in_doc[doc_index] + topic_count_a\n",
    "            TA_d = topicAssignment[doc_index]\n",
    "            \n",
    "            for word_index, word in enumerate(word_list):\n",
    "                calc_fast(TA_d, doc_index, word_index, wordToTopic, word, words_in_topic, total_unique_word_b, document_words, WID)\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gibs_it_sanic(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f gibs_it_sanic gibs_it_sanic(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2w = pd.DataFrame(wordToTopic).T\n",
    "t2w.columns = vocab\n",
    "\n",
    "top_words_per_topic = [\n",
    "    (t2w.iloc[k][t2w.iloc[k] > 0.001].sort_values(ascending=False).index.values.tolist()[:10])\n",
    "    for k in topics\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(top_words_per_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "to_sort = []\n",
    "for i in range(len(documentToTopic)):\n",
    "    to_sort.append((documentToTopic[i][0], i))\n",
    "\n",
    "to_sort.sort(key=lambda t: t[0], reverse=True)\n",
    "result1 = df.content[to_sort[0][1]]\n",
    "result2 = df.content[to_sort[1][1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
