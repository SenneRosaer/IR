{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.65 s, sys: 1.11 s, total: 2.76 s\n",
      "Wall time: 1.15 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tmtoon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tmtoon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import nltk\n",
    "import numpy\n",
    "import time\n",
    "import random\n",
    "import unidecode\n",
    "import string\n",
    "import heapq\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist, word_tokenize, WordNetLemmatizer\n",
    "from numba import njit, jit, cuda\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "%load_ext line_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMOUNT_OF_TOPICS = 10\n",
    "\n",
    "# top % of most common words are removed\n",
    "TOP_K_WORD_REMOVAL = 0.001\n",
    "\n",
    "# https://stats.stackexchange.com/questions/59684/what-are-typical-values-to-use-for-alpha-and-beta-in-latent-dirichlet-allocation/130876\n",
    "# High alpha: many topics per document [low : 1 : high]\n",
    "ALPHA            = 1\n",
    "# High beta: topic has many different words [0 - 1]\n",
    "BETA             = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.22 s, sys: 507 ms, total: 4.73 s\n",
      "Wall time: 4.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv(\"news_dataset.csv\")\n",
    "#df = df[:10000]\n",
    "df = df[df['content'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    washington eeaaaaeeeace    congressional repub...\n",
      "1    after the bullet shells get counted the blood ...\n",
      "2    when walt disneys bambi opened in  critics pra...\n",
      "3    death may be the great equalizer but it isnt n...\n",
      "4    seoul south korea     north koreas leader kim ...\n",
      "Name: content, dtype: object\n",
      "CPU times: user 10.3 s, sys: 707 ms, total: 11 s\n",
      "Wall time: 11 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove accents, special characters and lowercase\n",
    "# https://stackoverflow.com/questions/37926248/how-to-remove-accents-from-values-in-columns\n",
    "df.content = df.content.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8').str.lower().str.replace('[^a-z ]', '')\n",
    "print(df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5498\n",
      "3316\n",
      "CPU times: user 5min 32s, sys: 444 ms, total: 5min 33s\n",
      "Wall time: 5min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(len(df['content'][0]))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def remove_singles(content):\n",
    "    d = dict()\n",
    "    for word in content.split(' '):\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        if word in d:\n",
    "            d[word] += 1\n",
    "        else:\n",
    "            d[word] = 1\n",
    "    \n",
    "    new = \"\"\n",
    "    for word, val in d.items():\n",
    "        if val > 1 and word != '':\n",
    "            new += (word + \" \") * val\n",
    "            \n",
    "    return new\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_singles(content))\n",
    "print(len(df['content'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.29 ms, sys: 8 Âµs, total: 2.3 ms\n",
      "Wall time: 1.47 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove stop words and single letters\n",
    "# https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list.extend(list(string.ascii_lowercase))\n",
    "#pat = r'\\b(?:{})\\b'.format('|'.join(stopwords_list))\n",
    "#df['content'] = df['content'].str.replace(pat, '')\n",
    "#print(df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.9 s, sys: 1.28 s, total: 29.2 s\n",
      "Wall time: 29.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "fdict = dict()\n",
    "\n",
    "if TOP_K_WORD_REMOVAL > 0:\n",
    "\n",
    "    # remove common words\n",
    "    for i in df.content.str.split(' '):\n",
    "        i = list(filter(lambda x:x!=\"\" , i))\n",
    "        for word in i:\n",
    "            if word != '':\n",
    "                if word in fdict:\n",
    "                    fdict[word] += 1\n",
    "                else:\n",
    "                    fdict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\b(?:the|a|to|of|and|in|that|it|for|is|on|he|wa|with|said|his|at|i|have|by|but|be|trump|from|are|not|ha|this|an|they|who|about|we|you|or|their|were|more|had|she|her|one|will|would|people|been|there|year|u|state|when|if|which|what|say|all|new|out|so|time|up|president|after|like|than|also|can|s|clinton|no|some|just|him|other|do|our|them|my|into|could|over|republican|mr|how|american|because|first|even|now|two|woman|day|country|get|many|most|make|percent|campaign|last|those|government|way|house|only|told|me|think|company|these|right|police|dont|against|white|where|news|any|know|going|did|before|group|while|want|may|i|me|my|myself|we|our|ours|ourselves|you|you're|you've|you'll|you'd|your|yours|yourself|yourselves|he|him|his|himself|she|she's|her|hers|herself|it|it's|its|itself|they|them|their|theirs|themselves|what|which|who|whom|this|that|that'll|these|those|am|is|are|was|were|be|been|being|have|has|had|having|do|does|did|doing|a|an|the|and|but|if|or|because|as|until|while|of|at|by|for|with|about|against|between|into|through|during|before|after|above|below|to|from|up|down|in|out|on|off|over|under|again|further|then|once|here|there|when|where|why|how|all|any|both|each|few|more|most|other|some|such|no|nor|not|only|own|same|so|than|too|very|s|t|can|will|just|don|don't|should|should've|now|d|ll|m|o|re|ve|y|ain|aren|aren't|couldn|couldn't|didn|didn't|doesn|doesn't|hadn|hadn't|hasn|hasn't|haven|haven't|isn|isn't|ma|mightn|mightn't|mustn|mustn't|needn|needn't|shan|shan't|shouldn|shouldn't|wasn|wasn't|weren|weren't|won|won't|wouldn|wouldn't|a|b|c|d|e|f|g|h|i|j|k|l|m|n|o|p|q|r|s|t|u|v|w|x|y|z)\\b\n",
      "0    congressional congressional congressional     ...\n",
      "1                                                  ...\n",
      "2               walt walt walt disney disney disney...\n",
      "3    death death death death death death death deat...\n",
      "4    south south south south south south south kore...\n",
      "Name: content, dtype: object\n",
      "CPU times: user 1min 29s, sys: 4.03 ms, total: 1min 29s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "length = int(TOP_K_WORD_REMOVAL * len(fdict))\n",
    "top_k_words = heapq.nlargest(length, fdict, key=fdict.get)\n",
    "\n",
    "top_k_words.extend(stopwords_list)\n",
    "\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(top_k_words))\n",
    "print(pat)\n",
    "df['content'] = df['content'].str.replace(pat, '')\n",
    "print(df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    congressional congressional congressional come...\n",
      "1     blood blood blood window window see see see c...\n",
      "2     walt walt walt disney disney disney disney di...\n",
      "3    death death death death death death death deat...\n",
      "4    south south south south south south south kore...\n",
      "Name: content, dtype: object\n",
      "CPU times: user 7.57 s, sys: 0 ns, total: 7.57 s\n",
      "Wall time: 7.57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove multiple spaces in a\n",
    "df['content'] = df['content'].str.replace(r'\\s+', ' ')\n",
    "print(df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.02 s, sys: 0 ns, total: 4.02 s\n",
      "Wall time: 4.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vocab_set = set()\n",
    "\n",
    "def compute_dict(input):\n",
    "    result = dict()\n",
    "\n",
    "    for word in input.split(' '):\n",
    "        if word != '':\n",
    "            vocab_set.add(word)\n",
    "\n",
    "df['content'].apply(lambda content: compute_dict(content))\n",
    "vocab = list(vocab_set)\n",
    "\n",
    "tmp = {}\n",
    "count = 0\n",
    "for word in vocab:\n",
    "    tmp[word] = count\n",
    "    count += 1\n",
    "vocab = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.75 s, sys: 3.99 ms, total: 5.75 s\n",
      "Wall time: 5.75 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Toevoegen van extra column met index ineens\n",
    "def remove_dupes(input):\n",
    "    output = input.split(' ')\n",
    "    output = list(filter(lambda x:x!=\"\" , output))\n",
    "    for i, value in enumerate(output):\n",
    "        output[i] = vocab[value]\n",
    "    return np.array(output)\n",
    "df['content_list'] = df['content'].apply(lambda content: remove_dupes(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8 Âµs, sys: 0 ns, total: 8 Âµs\n",
      "Wall time: 10.7 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "numberOfTopics = AMOUNT_OF_TOPICS\n",
    "topics = [i for i in range(0, numberOfTopics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 57.1 s, sys: 88.2 ms, total: 57.2 s\n",
      "Wall time: 56.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wordToTopic = numpy.zeros((len(vocab), numberOfTopics), dtype=numpy.int64)\n",
    "documentToTopic = numpy.zeros((len(df.content), numberOfTopics), dtype=numpy.int64)\n",
    "topicAssignment = numpy.zeros(len(df.content_list), dtype=numpy.ndarray)\n",
    "words_in_topic = numpy.zeros(numberOfTopics, dtype=numpy.int64)\n",
    "words_in_doc = [0 for i in range(len(df.content))]\n",
    "for i, content in enumerate(df.content_list):\n",
    "    tmp = np.zeros(len(content), dtype=numpy.int64)\n",
    "    for word_index, word in enumerate(content):\n",
    "        topic = topics[int(np.random.rand()*numberOfTopics)]\n",
    "        tmp[word_index] = topic\n",
    "        wordToTopic[word, topic] += 1\n",
    "        documentToTopic[i, topic] += 1\n",
    "        words_in_topic[topic] += 1\n",
    "        words_in_doc[i] += 1\n",
    "    topicAssignment[i] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 Âµs, sys: 0 ns, total: 4 Âµs\n",
      "Wall time: 5.72 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "alpha = ALPHA\n",
    "beta = BETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 246 Âµs, sys: 2 Âµs, total: 248 Âµs\n",
      "Wall time: 252 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "@njit\n",
    "def calc_fast(rand_val, word_index, word, TA_d, doc_index, wordToTopic, words_in_topic, total_unique_word_b, document_words, WID):\n",
    "    \n",
    "        document_words[TA_d[word_index]] -= 1               \n",
    "        wordToTopic[word, TA_d[word_index]] -= 1\n",
    "        words_in_topic[TA_d[word_index]] -= 1\n",
    "\n",
    "        new_topic = (np.multiply(np.divide((wordToTopic[word] + beta), \n",
    "                                                       (words_in_topic + total_unique_word_b)), \n",
    "                                             np.divide((document_words + alpha), \n",
    "                                                       (WID))))\n",
    "\n",
    "        new_topic = np.divide(new_topic, np.sum(new_topic))\n",
    "        new_topic = np.cumsum(new_topic)\n",
    "        topic = 0\n",
    "        for i in range(0, numberOfTopics):\n",
    "            if rand_val[word_index] < new_topic[i]:\n",
    "                topic = i\n",
    "                break\n",
    "\n",
    "        new_topic = topic\n",
    "        TA_d[word_index] = new_topic\n",
    "        document_words[new_topic] += 1\n",
    "        wordToTopic[word, new_topic] += 1\n",
    "        words_in_topic[new_topic] += 1\n",
    "    \n",
    "def gibs_it_sanic(i):\n",
    "    total_unique_word_b = len(vocab) * beta\n",
    "    topic_count_a = numberOfTopics * alpha\n",
    "    for iteration in range(i):\n",
    "        print(\"Iteration:\", iteration + 1)\n",
    "        \n",
    "        for doc_index, word_list in enumerate(df.content_list):\n",
    "            document_words = documentToTopic[doc_index]\n",
    "            WID = words_in_doc[doc_index] + topic_count_a\n",
    "            TA_d = topicAssignment[doc_index]\n",
    "            \n",
    "            rand_val = np.random.random(len(word_list))\n",
    "            for word_index, word in enumerate(word_list):\n",
    "                calc_fast(rand_val, word_index, word, TA_d, doc_index, wordToTopic, words_in_topic, total_unique_word_b, document_words, WID)\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "CPU times: user 5min 24s, sys: 73.2 ms, total: 5min 25s\n",
      "Wall time: 5min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gibs_it_sanic(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f gibs_it_sanic gibs_it_sanic(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------  --------  -------  -------  ---------  --------  ------------  --------------  -------------\n",
      "court     law       case     justice  bill       federal   department    senate          investigation\n",
      "health    million   billion  car      bank       market    apple         plan            cost\n",
      "officer   city      attack   force    islamic    isi       iran          shooting        official\n",
      "party     cruz      rubio    vote     york       election  voter         democrat        donald\n",
      "game      team      player   season   de         back      world         play            sport\n",
      "school    student   sander   work     fox        study     child         voter           university\n",
      "official  russia    russian  attack   security   united    intelligence  administration  syria\n",
      "party     election  donald   thing    candidate  voter     political     vote            thats\n",
      "tax       china     north    policy   korea      business  trade         million         economic\n",
      "life      show      thing    black    film       im        family        story           man\n",
      "--------  --------  -------  -------  ---------  --------  ------------  --------------  -------------\n"
     ]
    }
   ],
   "source": [
    "t2w = pd.DataFrame(wordToTopic).T\n",
    "t2w.columns = vocab\n",
    "\n",
    "top_words_per_topic = [\n",
    "    (t2w.iloc[k][t2w.iloc[k] > 0.001].sort_values(ascending=False).index.values.tolist()[:9])\n",
    "    for k in topics\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(top_words_per_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "to_sort = []\n",
    "for i in range(len(documentToTopic)):\n",
    "    to_sort.append((documentToTopic[i][0], i))\n",
    "\n",
    "to_sort.sort(key=lambda t: t[0], reverse=True)\n",
    "result1 = df.content[to_sort[0][1]]\n",
    "result2 = df.content[to_sort[1][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
