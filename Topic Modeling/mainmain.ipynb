{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n",
      "CPU times: user 2.59 ms, sys: 0 ns, total: 2.59 ms\n",
      "Wall time: 2.06 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tmtoon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import nltk\n",
    "import numpy\n",
    "import time\n",
    "import random\n",
    "import unidecode\n",
    "import string\n",
    "import heapq\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist, word_tokenize\n",
    "from numba import njit, jit, cuda\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "%load_ext line_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMOUNT_OF_TOPICS = 10\n",
    "\n",
    "# top % of most common words are removed\n",
    "TOP_K_WORD_REMOVAL = 0.001\n",
    "\n",
    "# https://stats.stackexchange.com/questions/59684/what-are-typical-values-to-use-for-alpha-and-beta-in-latent-dirichlet-allocation/130876\n",
    "# High alpha: many topics per document [low : 1 : high]\n",
    "ALPHA            = 50 / AMOUNT_OF_TOPICS\n",
    "# High beta: topic has many different words [0 - 1]\n",
    "BETA             = 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.34 s, sys: 208 ms, total: 7.54 s\n",
      "Wall time: 7.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv(\"news_dataset.csv\")\n",
    "df = df[:10000]\n",
    "df = df[df['content'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    washington eeaaaaeeeace    congressional repub...\n",
      "1    after the bullet shells get counted the blood ...\n",
      "2    when walt disneys bambi opened in  critics pra...\n",
      "3    death may be the great equalizer but it isnt n...\n",
      "4    seoul south korea     north koreas leader kim ...\n",
      "Name: content, dtype: object\n",
      "CPU times: user 2.05 s, sys: 3.94 ms, total: 2.05 s\n",
      "Wall time: 2.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove accents, special characters and lowercase\n",
    "# https://stackoverflow.com/questions/37926248/how-to-remove-accents-from-values-in-columns\n",
    "df.content = df.content.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8').str.lower().str.replace('[^a-z ]', '')\n",
    "print(df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 532 µs, sys: 0 ns, total: 532 µs\n",
      "Wall time: 572 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove stop words and single letters\n",
    "# https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list.extend(list(string.ascii_lowercase))\n",
    "#pat = r'\\b(?:{})\\b'.format('|'.join(stopwords_list))\n",
    "#df['content'] = df['content'].str.replace(pat, '')\n",
    "#print(df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.92 s, sys: 180 ms, total: 7.1 s\n",
      "Wall time: 7.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove common words\n",
    "fdict = dict()\n",
    "for i in df.content.str.split(' '):\n",
    "    i = list(filter(lambda x:x!=\"\" , i))\n",
    "    for word in i:\n",
    "        if word != '':\n",
    "            if word in fdict:\n",
    "                fdict[word] += 1\n",
    "            else:\n",
    "                fdict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\b(?:the|to|a|of|and|in|that|for|on|he|mr|said|is|was|with|it|as|his|at|by|but|from|have|i|an|not|has|who|be|had|are|they|its|this|she|were|about|trump|her|their|or|would|more|one|you|been|we|new|which|when|people|will|after|like|what|out|there|if|some|also|all|up|than|ms|him|president|so|other|into|no|over|them|years|could|can|time|states|two|many|just|last|first|do|now|most|united|because|state|even|my|how|year|our|did|where|those|before|american|only|s|trumps|news|government|house|i|me|my|myself|we|our|ours|ourselves|you|you're|you've|you'll|you'd|your|yours|yourself|yourselves|he|him|his|himself|she|she's|her|hers|herself|it|it's|its|itself|they|them|their|theirs|themselves|what|which|who|whom|this|that|that'll|these|those|am|is|are|was|were|be|been|being|have|has|had|having|do|does|did|doing|a|an|the|and|but|if|or|because|as|until|while|of|at|by|for|with|about|against|between|into|through|during|before|after|above|below|to|from|up|down|in|out|on|off|over|under|again|further|then|once|here|there|when|where|why|how|all|any|both|each|few|more|most|other|some|such|no|nor|not|only|own|same|so|than|too|very|s|t|can|will|just|don|don't|should|should've|now|d|ll|m|o|re|ve|y|ain|aren|aren't|couldn|couldn't|didn|didn't|doesn|doesn't|hadn|hadn't|hasn|hasn't|haven|haven't|isn|isn't|ma|mightn|mightn't|mustn|mustn't|needn|needn't|shan|shan't|shouldn|shouldn't|wasn|wasn't|weren|weren't|won|won't|wouldn|wouldn't|a|b|c|d|e|f|g|h|i|j|k|l|m|n|o|p|q|r|s|t|u|v|w|x|y|z)\\b\n",
      "0    washington eeaaaaeeeace    congressional repub...\n",
      "1      bullet shells get counted  blood dries   vot...\n",
      "2     walt disneys bambi opened   critics praised  ...\n",
      "3    death may   great equalizer   isnt necessarily...\n",
      "4    seoul south korea     north koreas leader kim ...\n",
      "Name: content, dtype: object\n",
      "CPU times: user 20.2 s, sys: 100 µs, total: 20.2 s\n",
      "Wall time: 20.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "length = int(TOP_K_WORD_REMOVAL * len(fdict))\n",
    "top_k_words = heapq.nlargest(length, fdict, key=fdict.get)\n",
    "\n",
    "top_k_words.extend(stopwords_list)\n",
    "\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(top_k_words))\n",
    "print(pat)\n",
    "df['content'] = df['content'].str.replace(pat, '')\n",
    "print(df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 µs, sys: 0 ns, total: 5 µs\n",
      "Wall time: 9.54 µs\n",
      "0    washington eeaaaaeeeace congressional republic...\n",
      "1     bullet shells get counted blood dries votive ...\n",
      "2     walt disneys bambi opened critics praised spa...\n",
      "3    death may great equalizer isnt necessarily eve...\n",
      "4    seoul south korea north koreas leader kim sund...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "# remove multiple spaces in a\n",
    "df['content'] = df['content'].str.replace(r'\\s+', ' ')\n",
    "print(df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.78 s, sys: 63 µs, total: 1.78 s\n",
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vocab_set = set()\n",
    "\n",
    "def compute_dict(input):\n",
    "    result = dict()\n",
    "\n",
    "    for word in input.split(' '):\n",
    "        if word != '':\n",
    "            vocab_set.add(word)\n",
    "            if word in result:\n",
    "                result[word] += 1\n",
    "            else:\n",
    "                result[word] = 1\n",
    "    return result\n",
    "\n",
    "df['content_dict'] = df['content'].apply(lambda content: compute_dict(content))\n",
    "vocab = list(vocab_set)\n",
    "\n",
    "tmp ={}\n",
    "count = 0\n",
    "for word in vocab:\n",
    "    tmp[word] = count\n",
    "    count += 1\n",
    "vocab = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.67 s, sys: 17 µs, total: 2.67 s\n",
      "Wall time: 2.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Toevoegen van extra column met index ineens\n",
    "def remove_dupes(input):\n",
    "    output = list(set(input.split(' ')))\n",
    "    if '' in output:\n",
    "        output.remove('')\n",
    "    for i, value in enumerate(output):\n",
    "        output[i] = vocab[value]\n",
    "    return np.array(output)\n",
    "df['content_list'] = df['content'].apply(lambda content: remove_dupes(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7 µs, sys: 0 ns, total: 7 µs\n",
      "Wall time: 11.4 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "numberOfTopics = AMOUNT_OF_TOPICS\n",
    "topics = [i for i in range(0, numberOfTopics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45 s, sys: 28 ms, total: 45 s\n",
      "Wall time: 45.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wordToTopic = numpy.zeros((len(vocab), numberOfTopics), dtype=numpy.int64)\n",
    "documentToTopic = numpy.zeros((len(df.content), numberOfTopics))\n",
    "topicAssignment = []\n",
    "words_in_topic = numpy.zeros(numberOfTopics, dtype=numpy.int64)\n",
    "words_in_doc = [0 for i in range(len(df.content))]\n",
    "for i, content in enumerate(df.content_list):\n",
    "    tmp = []\n",
    "    for word in content:\n",
    "        topic = numpy.random.choice(topics)\n",
    "        tmp.append(topic)\n",
    "        wordToTopic[word, topic] += 1\n",
    "        documentToTopic[i, topic] += 1\n",
    "        words_in_topic[topic] += 1\n",
    "        words_in_doc[i] += 1\n",
    "    topicAssignment.append(np.array(tmp))\n",
    "    \n",
    "topicAssignment = np.array(topicAssignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 7.15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "alpha = ALPHA\n",
    "beta = BETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 623 µs, sys: 5 µs, total: 628 µs\n",
      "Wall time: 639 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "@njit\n",
    "def calc_fast(TA_d, doc_index, word_index, wordToTopic, word, words_in_topic, total_unique_word_b, document_words, WID):\n",
    "    document_words[TA_d[word_index]] -= 1\n",
    "    wordToTopic[word, TA_d[word_index]] -= 1\n",
    "    words_in_topic[TA_d[word_index]] -= 1\n",
    "                \n",
    "    new_topic = numpy.argmax(np.multiply(np.divide((wordToTopic[word] + beta), \n",
    "                                                   (words_in_topic + total_unique_word_b)), \n",
    "                                         np.divide((document_words + alpha), \n",
    "                                                   (WID))))\n",
    "\n",
    "    TA_d[word_index] = new_topic\n",
    "    document_words[new_topic] += 1\n",
    "    wordToTopic[word, new_topic] += 1\n",
    "    words_in_topic[new_topic] += 1\n",
    "    \n",
    "def gibs_it_sanic(i):\n",
    "    total_unique_word_b = len(vocab) * beta\n",
    "    topic_count_a = numberOfTopics * alpha\n",
    "    for iteration in range(i):\n",
    "        print(\"Iteration:\", iteration + 1)\n",
    "        \n",
    "        for doc_index, word_list in enumerate(df.content_list):\n",
    "            document_words = documentToTopic[doc_index]\n",
    "            WID = words_in_doc[doc_index] + topic_count_a\n",
    "            TA_d = topicAssignment[doc_index]\n",
    "            \n",
    "            for word_index, word in enumerate(word_list):\n",
    "                calc_fast(TA_d, doc_index, word_index, wordToTopic, word, words_in_topic, total_unique_word_b, document_words, WID)\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "CPU times: user 56 s, sys: 32 ms, total: 56.1 s\n",
      "Wall time: 56.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gibs_it_sanic(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f gibs_it_sanic gibs_it_sanic(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------  -------  -----------  --------------  -------  --------  ----------  --------  -------------\n",
      "began      cant     immediately  ground          speak    visit     criticized  de        pass\n",
      "know       good     might        white           help     members   look        whose     security\n",
      "well       day      told         thats           making   history   instead     done      together\n",
      "back       never    took         ways            billion  fall      career      served    trade\n",
      "think      right    left         others          yet      lot       change      trying    seen\n",
      "including  since    around       city            man      big       taken       thursday  international\n",
      "made       three    far          need            second   kind      social      reported  line\n",
      "say        times    officials    twitter         show     law       university  wanted    coming\n",
      "make       way      may          still           much     get       dont        another   going\n",
      "see        country  next         administration  federal  recently  likely      held      nations\n",
      "---------  -------  -----------  --------------  -------  --------  ----------  --------  -------------\n"
     ]
    }
   ],
   "source": [
    "t2w = pd.DataFrame(wordToTopic).T\n",
    "t2w.columns = vocab\n",
    "\n",
    "top_words_per_topic = [\n",
    "    (t2w.iloc[k][t2w.iloc[k] > 0.001].sort_values(ascending=False).index.values.tolist()[:9])\n",
    "    for k in topics\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(top_words_per_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "to_sort = []\n",
    "for i in range(len(documentToTopic)):\n",
    "    to_sort.append((documentToTopic[i][0], i))\n",
    "\n",
    "to_sort.sort(key=lambda t: t[0], reverse=True)\n",
    "result1 = df.content[to_sort[0][1]]\n",
    "result2 = df.content[to_sort[1][1]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
