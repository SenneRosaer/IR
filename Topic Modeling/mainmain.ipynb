{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.99 s, sys: 1.13 s, total: 3.12 s\n",
      "Wall time: 2.29 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tmtoon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tmtoon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import nltk\n",
    "import numpy\n",
    "import time\n",
    "import random\n",
    "import unidecode\n",
    "import string\n",
    "import heapq\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist, word_tokenize, WordNetLemmatizer\n",
    "from numba import njit, jit, cuda\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "%load_ext line_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AMOUNT_OF_TOPICS = 20\n",
    "\n",
    "# top % of most common words are removed\n",
    "TOP_K_WORD_REMOVAL = 0.001\n",
    "\n",
    "# https://stats.stackexchange.com/questions/59684/what-are-typical-values-to-use-for-alpha-and-beta-in-latent-dirichlet-allocation/130876\n",
    "# High alpha: many topics per document [low : 1 : high]\n",
    "ALPHA            = 50 / AMOUNT_OF_TOPICS\n",
    "# High beta: topic has many different words [0 - 1]\n",
    "BETA             = 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.32 s, sys: 967 ms, total: 9.29 s\n",
      "Wall time: 9.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv(\"news_dataset.csv\")\n",
    "#df = df[:25000]\n",
    "df = df[df['content'].notna()]\n",
    "\n",
    "df[\"documents\"] = df[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    washington eeaaaaeeeace    congressional repub...\n",
      "1    after the bullet shells get counted the blood ...\n",
      "2    when walt disneys bambi opened in  critics pra...\n",
      "3    death may be the great equalizer but it isnt n...\n",
      "4    seoul south korea     north koreas leader kim ...\n",
      "Name: content, dtype: object\n",
      "CPU times: user 16.9 s, sys: 977 ms, total: 17.9 s\n",
      "Wall time: 17.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove accents, special characters and lowercase\n",
    "# https://stackoverflow.com/questions/37926248/how-to-remove-accents-from-values-in-columns\n",
    "df.content = df.content.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8').str.lower().str.replace('[^a-z ]', '')\n",
    "print(df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5498\n",
      "3316\n",
      "CPU times: user 7min, sys: 630 ms, total: 7min\n",
      "Wall time: 7min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(len(df['content'][0]))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def remove_singles(content):\n",
    "    d = dict()\n",
    "    for word in content.split(' '):\n",
    "        word = lemmatizer.lemmatize(word)\n",
    "        if word in d:\n",
    "            d[word] += 1\n",
    "        else:\n",
    "            d[word] = 1\n",
    "    \n",
    "    new = \"\"\n",
    "    for word, val in d.items():\n",
    "        if val > 1 and word != '':\n",
    "            new += (word + \" \") * val\n",
    "            \n",
    "    return new\n",
    "\n",
    "df['content'] = df['content'].apply(lambda content: remove_singles(content))\n",
    "print(len(df['content'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.99 ms, sys: 3 µs, total: 2 ms\n",
      "Wall time: 1.26 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove stop words and single letters\n",
    "# https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list.extend(list(string.ascii_lowercase))\n",
    "#pat = r'\\b(?:{})\\b'.format('|'.join(stopwords_list))\n",
    "#df['content'] = df['content'].str.replace(pat, '')\n",
    "#print(df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.3 s, sys: 4.13 s, total: 34.4 s\n",
      "Wall time: 45.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "fdict = dict()\n",
    "\n",
    "if TOP_K_WORD_REMOVAL > 0:\n",
    "\n",
    "    # remove common words\n",
    "    for i in df.content.str.split(' '):\n",
    "        i = list(filter(lambda x:x!=\"\" , i))\n",
    "        for word in i:\n",
    "            if word != '':\n",
    "                if word in fdict:\n",
    "                    fdict[word] += 1\n",
    "                else:\n",
    "                    fdict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\b(?:the|a|to|of|and|in|that|it|for|is|on|he|wa|with|said|his|at|i|have|by|but|be|trump|from|are|not|ha|this|an|they|who|about|we|you|or|their|were|more|had|she|her|one|will|would|people|been|there|year|u|state|when|if|which|what|say|all|new|out|so|time|up|president|after|like|than|also|can|s|clinton|no|some|just|him|other|do|our|them|my|into|could|over|republican|mr|how|american|because|first|even|now|two|woman|day|country|get|many|most|make|percent|campaign|last|those|government|way|house|only|told|me|think|company|these|right|police|dont|against|white|where|news|any|know|going|did|before|group|while|want|may|i|me|my|myself|we|our|ours|ourselves|you|you're|you've|you'll|you'd|your|yours|yourself|yourselves|he|him|his|himself|she|she's|her|hers|herself|it|it's|its|itself|they|them|their|theirs|themselves|what|which|who|whom|this|that|that'll|these|those|am|is|are|was|were|be|been|being|have|has|had|having|do|does|did|doing|a|an|the|and|but|if|or|because|as|until|while|of|at|by|for|with|about|against|between|into|through|during|before|after|above|below|to|from|up|down|in|out|on|off|over|under|again|further|then|once|here|there|when|where|why|how|all|any|both|each|few|more|most|other|some|such|no|nor|not|only|own|same|so|than|too|very|s|t|can|will|just|don|don't|should|should've|now|d|ll|m|o|re|ve|y|ain|aren|aren't|couldn|couldn't|didn|didn't|doesn|doesn't|hadn|hadn't|hasn|hasn't|haven|haven't|isn|isn't|ma|mightn|mightn't|mustn|mustn't|needn|needn't|shan|shan't|shouldn|shouldn't|wasn|wasn't|weren|weren't|won|won't|wouldn|wouldn't|a|b|c|d|e|f|g|h|i|j|k|l|m|n|o|p|q|r|s|t|u|v|w|x|y|z)\\b\n",
      "0    congressional congressional congressional     ...\n",
      "1                                                  ...\n",
      "2               walt walt walt disney disney disney...\n",
      "3    death death death death death death death deat...\n",
      "4    south south south south south south south kore...\n",
      "Name: content, dtype: object\n",
      "CPU times: user 1min 50s, sys: 343 ms, total: 1min 51s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "length = int(TOP_K_WORD_REMOVAL * len(fdict))\n",
    "top_k_words = heapq.nlargest(length, fdict, key=fdict.get)\n",
    "\n",
    "top_k_words.extend(stopwords_list)\n",
    "\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(top_k_words))\n",
    "print(pat)\n",
    "df['content'] = df['content'].str.replace(pat, '')\n",
    "print(df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    congressional congressional congressional come...\n",
      "1     blood blood blood window window see see see c...\n",
      "2     walt walt walt disney disney disney disney di...\n",
      "3    death death death death death death death deat...\n",
      "4    south south south south south south south kore...\n",
      "Name: content, dtype: object\n",
      "CPU times: user 9.59 s, sys: 0 ns, total: 9.59 s\n",
      "Wall time: 9.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# remove multiple spaces in a\n",
    "df['content'] = df['content'].str.replace(r'\\s+', ' ')\n",
    "print(df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.03 s, sys: 10 µs, total: 5.03 s\n",
      "Wall time: 5.03 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vocab_set = set()\n",
    "\n",
    "def compute_dict(input):\n",
    "    result = dict()\n",
    "\n",
    "    for word in input.split(' '):\n",
    "        if word != '':\n",
    "            vocab_set.add(word)\n",
    "\n",
    "df['content'].apply(lambda content: compute_dict(content))\n",
    "vocab = list(vocab_set)\n",
    "\n",
    "tmp = {}\n",
    "count = 0\n",
    "for word in vocab:\n",
    "    tmp[word] = count\n",
    "    count += 1\n",
    "vocab = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.41 s, sys: 4.02 ms, total: 9.41 s\n",
      "Wall time: 9.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#Toevoegen van extra column met index ineens\n",
    "def remove_dupes(input):\n",
    "    output = input.split(' ')\n",
    "    output = list(filter(lambda x:x!=\"\" , output))\n",
    "    for i, value in enumerate(output):\n",
    "        output[i] = vocab[value]\n",
    "    return np.array(output)\n",
    "df['content_list'] = df['content'].apply(lambda content: remove_dupes(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 0 ns, total: 10 µs\n",
      "Wall time: 15 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "numberOfTopics = AMOUNT_OF_TOPICS\n",
    "topics = [i for i in range(0, numberOfTopics)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 17s, sys: 107 ms, total: 1min 17s\n",
      "Wall time: 1min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "wordToTopic = numpy.zeros((len(vocab), numberOfTopics), dtype=numpy.int64)\n",
    "documentToTopic = numpy.zeros((len(df.content), numberOfTopics), dtype=numpy.int64)\n",
    "topicAssignment = numpy.zeros(len(df.content_list), dtype=numpy.ndarray)\n",
    "words_in_topic = numpy.zeros(numberOfTopics, dtype=numpy.int64)\n",
    "words_in_doc = [0 for i in range(len(df.content))]\n",
    "for i, content in enumerate(df.content_list):\n",
    "    tmp = np.zeros(len(content), dtype=numpy.int64)\n",
    "    for word_index, word in enumerate(content):\n",
    "        topic = topics[int(np.random.rand()*numberOfTopics)]\n",
    "        tmp[word_index] = topic\n",
    "        wordToTopic[word, topic] += 1\n",
    "        documentToTopic[i, topic] += 1\n",
    "        words_in_topic[topic] += 1\n",
    "        words_in_doc[i] += 1\n",
    "    topicAssignment[i] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 8.11 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "alpha = ALPHA\n",
    "beta = BETA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 323 µs, sys: 0 ns, total: 323 µs\n",
      "Wall time: 329 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "@njit\n",
    "def calc_fast(rand_val, word_index, word, TA_d, doc_index, wordToTopic, words_in_topic, total_unique_word_b, document_words, WID):\n",
    "    \n",
    "        document_words[TA_d[word_index]] -= 1               \n",
    "        wordToTopic[word, TA_d[word_index]] -= 1\n",
    "        words_in_topic[TA_d[word_index]] -= 1\n",
    "\n",
    "        new_topic = (np.multiply(np.divide((wordToTopic[word] + beta), \n",
    "                                                       (words_in_topic + total_unique_word_b)), \n",
    "                                             np.divide((document_words + alpha), \n",
    "                                                       (WID))))\n",
    "\n",
    "        new_topic = np.divide(new_topic, np.sum(new_topic))\n",
    "        new_topic = np.cumsum(new_topic)\n",
    "        topic = 0\n",
    "        for i in range(0, numberOfTopics):\n",
    "            if rand_val[word_index] < new_topic[i]:\n",
    "                topic = i\n",
    "                break\n",
    "\n",
    "        new_topic = topic\n",
    "        TA_d[word_index] = new_topic\n",
    "        document_words[new_topic] += 1\n",
    "        wordToTopic[word, new_topic] += 1\n",
    "        words_in_topic[new_topic] += 1\n",
    "    \n",
    "def gibs_it_sanic(i):\n",
    "    total_unique_word_b = len(vocab) * beta\n",
    "    topic_count_a = numberOfTopics * alpha\n",
    "    for iteration in range(i):\n",
    "        print(\"Iteration:\", iteration + 1)\n",
    "        \n",
    "        for doc_index, word_list in enumerate(df.content_list):\n",
    "            document_words = documentToTopic[doc_index]\n",
    "            WID = words_in_doc[doc_index] + topic_count_a\n",
    "            TA_d = topicAssignment[doc_index]\n",
    "            \n",
    "            rand_val = np.random.random(len(word_list))\n",
    "            for word_index, word in enumerate(word_list):\n",
    "                calc_fast(rand_val, word_index, word, TA_d, doc_index, wordToTopic, words_in_topic, total_unique_word_b, document_words, WID)\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "Iteration: 11\n",
      "Iteration: 12\n",
      "Iteration: 13\n",
      "Iteration: 14\n",
      "Iteration: 15\n",
      "Iteration: 16\n",
      "Iteration: 17\n",
      "Iteration: 18\n",
      "Iteration: 19\n",
      "Iteration: 20\n",
      "Iteration: 21\n",
      "Iteration: 22\n",
      "Iteration: 23\n",
      "Iteration: 24\n",
      "Iteration: 25\n",
      "CPU times: user 17min 23s, sys: 998 ms, total: 17min 24s\n",
      "Wall time: 17min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gibs_it_sanic(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %lprun -f gibs_it_sanic gibs_it_sanic(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------  -------------  ----------  ----------  -----------  ------------  ---------  ---------  ------------\n",
      "officer   twitter        medium      gun         video        shooting      breitbart  fox        march\n",
      "official  investigation  email       department  report       intelligence  fbi        security   russian\n",
      "law       court          case        federal     immigration  order         judge      justice    crime\n",
      "black     america        food        history     political    world         movement   much       today\n",
      "attack    killed         french      authority   france       man           migrant    refugee    germany\n",
      "show      film           star        music       song         movie         world      series     character\n",
      "force     military       war         syria       russia       isi           united     iran       syrian\n",
      "school    student        university  men         college      child         girl       education  young\n",
      "muslim    speech         world       israel      church       america       religious  christian  west\n",
      "home      family         city        child       mother       life          building   water      street\n",
      "team      game           player      de          season       second        play       back       three\n",
      "tax       plan           billion     market      job          price         health     bank       care\n",
      "thing     thats          im          really      go           see           something  much       lot\n",
      "million   business       york        money       employee     office        according  work       foundation\n",
      "party     election       voter       candidate   donald       vote          sander     hillary    presidential\n",
      "health    study          drug        found       research     medical       patient    doctor     hospital\n",
      "north     china          united      south       korea        deal          trade      nuclear    world\n",
      "change    climate        water       refugee     energy       flight        space      agency     fire\n",
      "car       facebook       apple       technology  user         data          google     product    service\n",
      "obama     bill           senate      democrat    congress     senator       ryan       member     vote\n",
      "--------  -------------  ----------  ----------  -----------  ------------  ---------  ---------  ------------\n"
     ]
    }
   ],
   "source": [
    "t2w = pd.DataFrame(wordToTopic).T\n",
    "t2w.columns = vocab\n",
    "\n",
    "top_words_per_topic = [\n",
    "    (t2w.iloc[k][t2w.iloc[k] > 0.001].sort_values(ascending=False).index.values.tolist()[:9])\n",
    "    for k in topics\n",
    "]\n",
    "\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(top_words_per_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------  ------  ------  ------  ------\n",
      "138437   28739  139018   13282   73706\n",
      "132091  133097  100946   38569      89\n",
      "138437   36980   67991   71781  140995\n",
      "   270  130648   73143   77505   53226\n",
      " 36060   25008   24865   21849    8796\n",
      " 17316  104846  129968  108026   56867\n",
      " 63068   61499   36060   69997  127361\n",
      " 53693   53566   53580   53427   53790\n",
      "126572   51165   53551    4659   71452\n",
      " 83788   69253   87362   67991   53559\n",
      " 66870   67980   70780   66660  136425\n",
      " 70605  126885  126345  112542  130005\n",
      "136014    1162   32721    6262  131434\n",
      " 50498   50967   50581   51104   51344\n",
      " 18159   52518   55656   56190   17436\n",
      " 50716  106921    3423   50027    3435\n",
      " 23284   54137   26027   56268   53992\n",
      "101692   51665   15664   43807   56974\n",
      "103298   68442   68526    6407  100502\n",
      " 53924   72247   50319    1748   71781\n",
      "------  ------  ------  ------  ------\n"
     ]
    }
   ],
   "source": [
    "t2d = pd.DataFrame(documentToTopic.T)\n",
    "\n",
    "top_docs_per_topic = [\n",
    "    (t2d.iloc[k][t2d.iloc[k] > 0.001].sort_values(ascending=False).index.values.tolist()[:5])\n",
    "    for k in topics\n",
    "]\n",
    "\n",
    "\n",
    "from tabulate import tabulate\n",
    "print(tabulate(top_docs_per_topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36.25233180059962\n",
      "CPU times: user 2min 28s, sys: 15.1 ms, total: 2min 28s\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "docs = len(df.content_list)\n",
    "score = 0\n",
    "for tops in top_words_per_topic:\n",
    "    for i in range(0, len(tops)-2):\n",
    "        w_i = tops[i]\n",
    "        w_j = tops[i+1]\n",
    "        \n",
    "        c1 = 0\n",
    "        c2 = 0\n",
    "        c3 = 0\n",
    "        for item in df.content_list:\n",
    "            tmp = 0\n",
    "            if vocab[w_i] in item:\n",
    "                c1 += 1\n",
    "                tmp += 1\n",
    "            if vocab[w_j] in item:\n",
    "                c2 += 1\n",
    "                tmp += 1\n",
    "            if tmp == 2:\n",
    "                c3 += 1\n",
    "        \n",
    "        a = c3 / docs\n",
    "        b = c1 / docs\n",
    "        c = c2 / docs\n",
    "        if b != 0 and c != 0:\n",
    "            score -= np.log(a / b * c)\n",
    "score /= len(top_words_per_topic)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[138437  28739 139018 ... 105805  86492  67004]\n",
      " [132091 133097 100946 ...  63950  44649 129440]\n",
      " [138437  36980  67991 ...  71620  96141  66938]\n",
      " ...\n",
      " [101692  51665  15664 ... 134701  95468 129016]\n",
      " [103298  68442  68526 ...  35138 104626 138394]\n",
      " [ 53924  72247  50319 ... 126506   2669  52058]]\n",
      "[[138437 132091 138437 ... 101692 103298  53924]\n",
      " [ 28739 133097  36980 ...  51665  68442  72247]\n",
      " [139018 100946  67991 ...  15664  68526  50319]\n",
      " ...\n",
      " [105805  63950  71620 ... 134701  35138 126506]\n",
      " [ 86492  44649  96141 ...  95468 104626   2669]\n",
      " [ 67004 129440  66938 ... 129016 138394  52058]]\n"
     ]
    }
   ],
   "source": [
    "t2d = pd.DataFrame(documentToTopic.T)\n",
    "\n",
    "top_docs_per_topic = [\n",
    "    (t2d.iloc[k][t2d.iloc[k] > 0.001].sort_values(ascending=False).index.values.tolist()[:100])\n",
    "    for k in topics\n",
    "]\n",
    "\n",
    "a = np.asarray(top_docs_per_topic)\n",
    "a = a.transpose()\n",
    "np.savetxt(\"topic_ids_\", a, delimiter=\",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
