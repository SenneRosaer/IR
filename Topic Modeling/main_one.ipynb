{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/tmtoon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tmtoon/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "Iteration: 11\n",
      "Iteration: 12\n",
      "Iteration: 13\n",
      "Iteration: 14\n",
      "Iteration: 15\n",
      "Iteration: 16\n",
      "Iteration: 17\n",
      "Iteration: 18\n",
      "Iteration: 19\n",
      "Iteration: 20\n",
      "Iteration: 21\n",
      "Iteration: 22\n",
      "Iteration: 23\n",
      "Iteration: 24\n",
      "Iteration: 25\n",
      "------  --------  --------  -------  --------  ---------  -------  --------  ---------\n",
      "world   show      film      around   water     story      book     see       food\n",
      "party   election  obama     donald   vote      political  voter    democrat  candidate\n",
      "court   million   case      federal  business  law        team     game      billion\n",
      "school  thing     health    child    life      work       student  im        family\n",
      "united  attack    official  city     officer   security   war      north     force\n",
      "------  --------  --------  -------  --------  ---------  -------  --------  ---------\n",
      "------  ------  -----  -----  -----\n",
      "129968     270   7729  23284  68824\n",
      " 56190  138437  55656  18159  55118\n",
      " 70605  136425  50441  50362  51104\n",
      " 66870   67980  70780  53528  53790\n",
      " 36060   25008  24865  28739  73706\n",
      "------  ------  -----  -----  -----\n",
      "score: 30.819858277980575 k: 5 removed: 0.001 alpha: 10.0 beta: 0.1\n",
      "\n",
      "\n",
      "\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "Iteration: 11\n",
      "Iteration: 12\n",
      "Iteration: 13\n",
      "Iteration: 14\n",
      "Iteration: 15\n",
      "Iteration: 16\n",
      "Iteration: 17\n",
      "Iteration: 18\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import nltk\n",
    "import numpy\n",
    "import time\n",
    "import random\n",
    "import unidecode\n",
    "import string\n",
    "import heapq\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist, word_tokenize, WordNetLemmatizer\n",
    "from numba import njit, jit, cuda\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "%load_ext line_profiler\n",
    "\n",
    "for a in [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]:\n",
    "    AMOUNT_OF_TOPICS = a\n",
    "\n",
    "    # top % of most common words are removed\n",
    "    TOP_K_WORD_REMOVAL = 0.001\n",
    "\n",
    "    # https://stats.stackexchange.com/questions/59684/what-are-typical-values-to-use-for-alpha-and-beta-in-latent-dirichlet-allocation/130876\n",
    "    # High alpha: many topics per document [low : 1 : high]\n",
    "    ALPHA            = 50 / AMOUNT_OF_TOPICS\n",
    "    # High beta: topic has many different words [0 - 1]\n",
    "    BETA             = 0.1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df = pd.read_csv(\"news_dataset.csv\")\n",
    "    #df = df[:10000]\n",
    "    df = df[df['content'].notna()]\n",
    "\n",
    "    df[\"documents\"] = df[\"content\"]\n",
    "\n",
    "\n",
    "\n",
    "    # remove accents, special characters and lowercase\n",
    "    # https://stackoverflow.com/questions/37926248/how-to-remove-accents-from-values-in-columns\n",
    "    df.content = df.content.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8').str.lower().str.replace('[^a-z ]', '')\n",
    "    #print(df.content.head())\n",
    "\n",
    "\n",
    "    #print(len(df['content'][0]))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    def remove_singles(content):\n",
    "        d = dict()\n",
    "        for word in content.split(' '):\n",
    "            word = lemmatizer.lemmatize(word)\n",
    "            if word in d:\n",
    "                d[word] += 1\n",
    "            else:\n",
    "                d[word] = 1\n",
    "\n",
    "        new = \"\"\n",
    "        for word, val in d.items():\n",
    "            if val > 1 and word != '':\n",
    "                new += (word + \" \") * val\n",
    "\n",
    "        return new\n",
    "\n",
    "    df['content'] = df['content'].apply(lambda content: remove_singles(content))\n",
    "    #print(len(df['content'][0]))\n",
    "\n",
    "\n",
    "\n",
    "    # remove stop words and single letters\n",
    "    # https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    stopwords_list.extend(list(string.ascii_lowercase))\n",
    "    #pat = r'\\b(?:{})\\b'.format('|'.join(stopwords_list))\n",
    "    #df['content'] = df['content'].str.replace(pat, '')\n",
    "    #print(df.content.head())\n",
    "\n",
    "\n",
    "\n",
    "    fdict = dict()\n",
    "\n",
    "    if TOP_K_WORD_REMOVAL > 0:\n",
    "\n",
    "        # remove common words\n",
    "        for i in df.content.str.split(' '):\n",
    "            i = list(filter(lambda x:x!=\"\" , i))\n",
    "            for word in i:\n",
    "                if word != '':\n",
    "                    if word in fdict:\n",
    "                        fdict[word] += 1\n",
    "                    else:\n",
    "                        fdict[word] = 1\n",
    "\n",
    "\n",
    "    length = int(TOP_K_WORD_REMOVAL * len(fdict))\n",
    "    top_k_words = heapq.nlargest(length, fdict, key=fdict.get)\n",
    "\n",
    "    top_k_words.extend(stopwords_list)\n",
    "\n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(top_k_words))\n",
    "    #print(pat)\n",
    "    df['content'] = df['content'].str.replace(pat, '')\n",
    "    #print(df.content.head())\n",
    "\n",
    "\n",
    "    # remove multiple spaces in a\n",
    "    df['content'] = df['content'].str.replace(r'\\s+', ' ')\n",
    "    #print(df.content.head())\n",
    "\n",
    "\n",
    "    vocab_set = set()\n",
    "\n",
    "    def compute_dict(input):\n",
    "        result = dict()\n",
    "\n",
    "        for word in input.split(' '):\n",
    "            if word != '':\n",
    "                vocab_set.add(word)\n",
    "\n",
    "    df['content'].apply(lambda content: compute_dict(content))\n",
    "    vocab = list(vocab_set)\n",
    "\n",
    "    tmp = {}\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        tmp[word] = count\n",
    "        count += 1\n",
    "    vocab = tmp\n",
    "\n",
    "\n",
    "\n",
    "    #Toevoegen van extra column met index ineens\n",
    "    def remove_dupes(input):\n",
    "        output = input.split(' ')\n",
    "        output = list(filter(lambda x:x!=\"\" , output))\n",
    "        for i, value in enumerate(output):\n",
    "            output[i] = vocab[value]\n",
    "        return np.array(output)\n",
    "    df['content_list'] = df['content'].apply(lambda content: remove_dupes(content))\n",
    "\n",
    "\n",
    "\n",
    "    numberOfTopics = AMOUNT_OF_TOPICS\n",
    "    topics = [i for i in range(0, numberOfTopics)]\n",
    "\n",
    "\n",
    "\n",
    "    wordToTopic = numpy.zeros((len(vocab), numberOfTopics), dtype=numpy.int64)\n",
    "    documentToTopic = numpy.zeros((len(df.content), numberOfTopics), dtype=numpy.int64)\n",
    "    topicAssignment = numpy.zeros(len(df.content_list), dtype=numpy.ndarray)\n",
    "    words_in_topic = numpy.zeros(numberOfTopics, dtype=numpy.int64)\n",
    "    words_in_doc = [0 for i in range(len(df.content))]\n",
    "    for i, content in enumerate(df.content_list):\n",
    "        tmp = np.zeros(len(content), dtype=numpy.int64)\n",
    "        for word_index, word in enumerate(content):\n",
    "            topic = topics[int(np.random.rand()*numberOfTopics)]\n",
    "            tmp[word_index] = topic\n",
    "            wordToTopic[word, topic] += 1\n",
    "            documentToTopic[i, topic] += 1\n",
    "            words_in_topic[topic] += 1\n",
    "            words_in_doc[i] += 1\n",
    "        topicAssignment[i] = tmp\n",
    "\n",
    "\n",
    "    alpha = ALPHA\n",
    "    beta = BETA\n",
    "\n",
    "\n",
    "    @njit\n",
    "    def calc_fast(rand_val, word_index, word, TA_d, doc_index, wordToTopic, words_in_topic, total_unique_word_b, document_words, WID):\n",
    "\n",
    "            document_words[TA_d[word_index]] -= 1               \n",
    "            wordToTopic[word, TA_d[word_index]] -= 1\n",
    "            words_in_topic[TA_d[word_index]] -= 1\n",
    "\n",
    "            new_topic = (np.multiply(np.divide((wordToTopic[word] + beta), \n",
    "                                                           (words_in_topic + total_unique_word_b)), \n",
    "                                                 np.divide((document_words + alpha), \n",
    "                                                           (WID))))\n",
    "\n",
    "            new_topic = np.divide(new_topic, np.sum(new_topic))\n",
    "            new_topic = np.cumsum(new_topic)\n",
    "            topic = 0\n",
    "            for i in range(0, numberOfTopics):\n",
    "                if rand_val[word_index] < new_topic[i]:\n",
    "                    topic = i\n",
    "                    break\n",
    "\n",
    "            new_topic = topic\n",
    "            TA_d[word_index] = new_topic\n",
    "            document_words[new_topic] += 1\n",
    "            wordToTopic[word, new_topic] += 1\n",
    "            words_in_topic[new_topic] += 1\n",
    "\n",
    "    def gibs_it_sanic(i):\n",
    "        total_unique_word_b = len(vocab) * beta\n",
    "        topic_count_a = numberOfTopics * alpha\n",
    "        for iteration in range(i):\n",
    "            print(\"Iteration:\", iteration + 1)\n",
    "\n",
    "            for doc_index, word_list in enumerate(df.content_list):\n",
    "                document_words = documentToTopic[doc_index]\n",
    "                WID = words_in_doc[doc_index] + topic_count_a\n",
    "                TA_d = topicAssignment[doc_index]\n",
    "\n",
    "                rand_val = np.random.random(len(word_list))\n",
    "                for word_index, word in enumerate(word_list):\n",
    "                    calc_fast(rand_val, word_index, word, TA_d, doc_index, wordToTopic, words_in_topic, total_unique_word_b, document_words, WID)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    gibs_it_sanic(25)\n",
    "\n",
    "    t2w = pd.DataFrame(wordToTopic).T\n",
    "    t2w.columns = vocab\n",
    "\n",
    "    top_words_per_topic = [\n",
    "        (t2w.iloc[k][t2w.iloc[k] > 0.001].sort_values(ascending=False).index.values.tolist()[:9])\n",
    "        for k in topics\n",
    "    ]\n",
    "\n",
    "\n",
    "    from tabulate import tabulate\n",
    "    print(tabulate(top_words_per_topic))\n",
    "\n",
    "    t2d = pd.DataFrame(documentToTopic.T)\n",
    "\n",
    "    top_docs_per_topic = [\n",
    "        (t2d.iloc[k][t2d.iloc[k] > 0.001].sort_values(ascending=False).index.values.tolist()[:5])\n",
    "        for k in topics\n",
    "    ]\n",
    "\n",
    "\n",
    "    from tabulate import tabulate\n",
    "    print(tabulate(top_docs_per_topic))\n",
    "\n",
    "\n",
    "\n",
    "    docs = len(df.content_list)\n",
    "    score = 0\n",
    "    for tops in top_words_per_topic:\n",
    "        for i in range(0, len(tops)-2):\n",
    "            w_i = tops[i]\n",
    "            w_j = tops[i+1]\n",
    "\n",
    "            c1 = 0\n",
    "            c2 = 0\n",
    "            c3 = 0\n",
    "            for item in df.content_list:\n",
    "                tmp = 0\n",
    "                if vocab[w_i] in item:\n",
    "                    c1 += 1\n",
    "                    tmp += 1\n",
    "                if vocab[w_j] in item:\n",
    "                    c2 += 1\n",
    "                    tmp += 1\n",
    "                if tmp == 2:\n",
    "                    c3 += 1\n",
    "\n",
    "            if c1 != 0 and c2 != 0:\n",
    "                score -= np.log((c3 / docs) / (c1 / docs) * (c2 / docs))\n",
    "    score /= len(top_words_per_topic)\n",
    "    print(\"score:\", score, \"k:\", AMOUNT_OF_TOPICS, \"removed:\", TOP_K_WORD_REMOVAL, \"alpha:\", ALPHA, \"beta:\", BETA)\n",
    "    print()\n",
    "    print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
